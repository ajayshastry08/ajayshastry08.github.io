<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PA-Dino</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <link rel="apple-touch-icon" href="apple-touch-icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Didact+Gothic" />

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body font-family: 'Didact Gothic'>
    <div class="container">
        <div class="row">
            <h2 class="col-md-12 text-center" font-family: 'Didact Gothic'>
                Parts based Attention for Highly Occluded Pedestrian Detection with Transformers
                </br>
                <small>
                    ICIP 2023
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
						K.N Ajay Shastry
                      </br>IIT Delhi
                    </li>
                    <li>
                        Jayesh Chaudhari
                        </br>IIT Delhi
                    <li>
                        Daksh Thapar
                        </br>IIT Mandi
                    </li>
                    <li>
                        Aditya Nigam
                      </br>IIT Mandi
                    </li>
                    <li>
                        <a href="https://www.cse.iitd.ac.in/~chetan">
							Chetan Arora
                        </a>
                        </br>IIT Delhi
                    </li>
                </ul>
            </div>
        </div>
        <div class="row" align="middle">
            <div class="btn-group" role="group" aria-label="Top menu" align="middle">
                <a class="btn btn-primary" href="">Paper</a>
                <a class="btn btn-primary" href="https://github.com/ajayshastry08/Parts-aware-DINO">Code</a>
            </div>
        </div>
        
        <br><br>

        <div class="row" id="header_img" align="middle">
            <image src="pa-dino-images/main_block_diagram.jpg" class="img-responsive" alt="overview" width="700px">
        </div>
		
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    Abstract
                </h4>
                <p class="text-justify" font-family: 'Didact Gothic'>
                    Despite the significant progress made in pedestrian detection
                    in last decade, detecting pedestrians under heavy occlusion
                    still remains a challenging problem. In state of the art (SOTA),
                    convolutional neural network (CNN) based models, the reason
                    is attributed to non-maximal-suppression (NMS), which often
                    erroneously deletes true positives when one pedestrian is oc-
                    cluding other. SOTA transformer based models do not have
                    such NMS step, yet fail to detect highly occluded pedestrians.
                    In this paper, we study the reasons for such failures. We
                    observe that such models first predict key-points, and then
                    compute the attention at the specific key-points. Our analysis
                    reveals that the key-points do not have any preference towards
                    semantically important body parts. Under heavy occlusion,
                    such key-points end up attending to non-discriminative re-
                    gions or background, leading to false negatives. We take
                    inspiration from the conventional wisdom of detecting ob-
                    jects using their parts, and bias the attention of proposed
                    transformer architecture towards semantically important, and
                    highly discriminative human body parts. The intervention
                    leads to SOTA results on benchmark Citypersons and Caltech
                    datasets, achieving 30.75%, and 32.96% miss-rate (lower is
                    better) respectively, against 32.6%, and 38.2% by the current
                    SOTA
                </p>
            </div>
        </div>
		
		 <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    Performance Comparision
                </h4>
                
                <div class="row" id="result_img" align="middle">
                    <image src="pa-dino-images/best_performance_comparision.jpg" class="img-responsive" alt="results" width="600px">
                </div>
		
            </div>
        </div>
	   
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4>
                    BibTeX (Citation)
                </h4>
				<pre style="white-space: pre-wrap; background: hsl(220, 50%, 95%); font-size: 11px">
@@inproceedings{basu2022unsupervised,
  title={Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining},
  author={Basu, Soumen and Singla, Somanshu and Gupta, Mayank and Rana, Pratyaksha and Gupta, Pankaj and Arora, Chetan},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={423--433},
  year={2022},
  organization={Springer}
}			
                </pre>
			</div>
        </div> -->
        <!--div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h4 font-family: 'Didact Gothic'>
                    Acknowledgements
                </h5>
                <p class="text-justify" font-family: 'Didact Gothic'>
                This work was supported by ???.
                </p>
            </div>
        </div-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify" font-family: 'Didact Gothic'>
                <center style="font-size:10px"><b>Credits:</b> Template of this webpage from <a href="http://www.mgharbi.com/">
                    here.
                </a></center>
                </p>
            </div>
        </div>
    </div>
</body>
</html>